{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OfqQKY2uJep",
        "outputId": "b37f7f1a-c966-49ff-c4cd-7a1423afe3ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  4 15:32:32 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXRyS8eMvXek",
        "outputId": "82d850d2-428f-4ec3-c0da-ee101f41205d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.20\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjqgMPEnvbH_",
        "outputId": "f5aa4d77-9d5f-4b18-a194-45e17e5acb65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 29.1/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ],
      "metadata": {
        "id": "r9jHoS9yvepw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!yolo task=detect mode=predict model=yolov8n.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcQuzYAZwOBq",
        "outputId": "38d2c2a5-bd44-4c34-9cee-b0596a205cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to yolov8n.pt...\n",
            "100% 6.23M/6.23M [00:00<00:00, 122MB/s]\n",
            "\n",
            "2024-04-04 15:36:41.125095: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 15:36:41.125145: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 15:36:41.126715: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 15:36:42.203626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
            "Downloading https://media.roboflow.com/notebooks/examples/dog.jpeg to dog.jpeg...\n",
            "100% 104k/104k [00:00<00:00, 32.5MB/s]\n",
            "WARNING âš ï¸ NMS time limit 0.550s exceeded\n",
            "image 1/1 /content/dog.jpeg: 640x384 1 person, 1 car, 1 dog, 87.5ms\n",
            "Speed: 0.8ms pre-process, 87.5ms inference, 950.0ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60n0WXViwSRh",
        "outputId": "cce0d91e-5255-4337-975c-a5603056477e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"ssMrVQn5ilwJPvDa46K6\")\n",
        "project = rf.workspace(\"learning-fk4al\").project(\"actual-project\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"yolov8\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YJKSbL60wdvR",
        "outputId": "d3635e58-4aa8-4867-bf57-d549d43aba5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.26-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Collecting idna==2.10 (from roboflow)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Collecting requests-toolbelt (from roboflow)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic (from roboflow)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.50.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Installing collected packages: python-magic, python-dotenv, opencv-python-headless, idna, cycler, chardet, certifi, requests-toolbelt, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.9.0.80\n",
            "    Uninstalling opencv-python-headless-4.9.0.80:\n",
            "      Successfully uninstalled opencv-python-headless-4.9.0.80\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2024.2.2\n",
            "    Uninstalling certifi-2024.2.2:\n",
            "      Successfully uninstalled certifi-2024.2.2\n",
            "Successfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 python-dotenv-1.0.1 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "chardet",
                  "cv2",
                  "cycler",
                  "idna"
                ]
              },
              "id": "c2b9bd67264d4409914e494690ef3dd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.0.20, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Actual-project-1 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39152/39152 [00:00<00:00, 40727.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Actual-project-1 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 834/834 [00:00<00:00, 4800.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=train model=yolov8m.pt data=/content/Actual-project-1/data.yaml epochs=25 imgsz=640 plots=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2NKP0OWwv4j",
        "outputId": "a7234a5c-392b-428b-ad19-7a68bf5e7c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt to yolov8m.pt...\n",
            "100% 49.7M/49.7M [00:00<00:00, 67.0MB/s]\n",
            "\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=/content/Actual-project-1/data.yaml, epochs=25, patience=50, batch=16, imgsz=640, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.1, cfg=None, v5loader=False, save_dir=runs/detect/train\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 32.4MB/s]\n",
            "2024-04-04 15:45:39.211099: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 15:45:39.211165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 15:45:39.212977: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 15:45:40.313240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.Conv                  [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.Conv                  [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.C2f                   [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.Conv                  [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.C2f                   [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.Conv                  [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.C2f                   [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.Conv                  [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.C2f                   [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.SPPF                  [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.C2f                   [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.C2f                   [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.Conv                  [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.C2f                   [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.Conv                  [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.C2f                   [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3777433  ultralytics.nn.modules.Detect                [3, [192, 384, 576]]          \n",
            "Model summary: 295 layers, 25858057 parameters, 25858041 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 469/475 items from pretrained weights\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.001), 83 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Actual-project-1/train/labels... 360 images, 0 backgrounds, 0 corrupt: 100% 360/360 [00:00<00:00, 1943.02it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/Actual-project-1/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Actual-project-1/valid/labels... 16 images, 0 backgrounds, 0 corrupt: 100% 16/16 [00:00<00:00, 2029.11it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/Actual-project-1/valid/labels.cache\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/25      7.53G      2.399      3.716       2.16         49        640: 100% 23/23 [00:24<00:00,  1.07s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:01<00:00,  1.79s/it]\n",
            "                   all         16         55      0.246      0.522      0.239      0.122\n",
            "     person-with-phone         16         18      0.379      0.778      0.336       0.16\n",
            "  person-without-phone         16         19      0.359      0.789      0.368      0.202\n",
            "         phone-posture         16         18          0          0     0.0133     0.0027\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/25      7.53G      2.218      2.726       1.97         56        640: 100% 23/23 [00:19<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.72it/s]\n",
            "                   all         16         55       0.28      0.606      0.349      0.191\n",
            "     person-with-phone         16         18      0.344      0.833      0.445      0.276\n",
            "  person-without-phone         16         19      0.294      0.842      0.418      0.249\n",
            "         phone-posture         16         18      0.202      0.142      0.184     0.0471\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/25      7.54G       2.04      2.414      1.822         63        640: 100% 23/23 [00:18<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.78it/s]\n",
            "                   all         16         55      0.353      0.372      0.345        0.2\n",
            "     person-with-phone         16         18      0.449      0.444      0.529       0.35\n",
            "  person-without-phone         16         19      0.443      0.461      0.401      0.218\n",
            "         phone-posture         16         18      0.166      0.211      0.105     0.0311\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/25      7.54G      2.014      2.521      1.742         38        640: 100% 23/23 [00:19<00:00,  1.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.74it/s]\n",
            "                   all         16         55      0.615      0.278      0.287      0.139\n",
            "     person-with-phone         16         18      0.446        0.5      0.431      0.247\n",
            "  person-without-phone         16         19          1          0      0.179      0.099\n",
            "         phone-posture         16         18      0.398      0.333      0.252     0.0706\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/25      7.94G      2.005      2.402      1.759         63        640: 100% 23/23 [00:18<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.76it/s]\n",
            "                   all         16         55      0.312      0.487      0.341      0.173\n",
            "     person-with-phone         16         18      0.345      0.611      0.403      0.205\n",
            "  person-without-phone         16         19      0.395      0.684      0.487      0.272\n",
            "         phone-posture         16         18      0.196      0.167      0.132     0.0421\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/25      7.94G      1.994      2.257      1.742         44        640: 100% 23/23 [00:19<00:00,  1.18it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.69it/s]\n",
            "                   all         16         55      0.283      0.475      0.328      0.161\n",
            "     person-with-phone         16         18      0.454      0.611      0.501      0.261\n",
            "  person-without-phone         16         19      0.324      0.758      0.373      0.193\n",
            "         phone-posture         16         18     0.0708     0.0556      0.108     0.0278\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/25      7.94G      2.009      2.248      1.744         28        640: 100% 23/23 [00:18<00:00,  1.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.71it/s]\n",
            "                   all         16         55      0.322      0.618      0.433      0.209\n",
            "     person-with-phone         16         18      0.423      0.722      0.529      0.274\n",
            "  person-without-phone         16         19      0.417      0.632      0.471      0.247\n",
            "         phone-posture         16         18      0.125        0.5      0.298      0.107\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/25      7.94G      2.009      2.171       1.72         65        640: 100% 23/23 [00:20<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.71it/s]\n",
            "                   all         16         55      0.403      0.562      0.458      0.226\n",
            "     person-with-phone         16         18      0.405      0.722        0.6      0.318\n",
            "  person-without-phone         16         19      0.472      0.632      0.439      0.233\n",
            "         phone-posture         16         18      0.331      0.333      0.335      0.127\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/25      7.94G      2.024      2.084      1.724         58        640: 100% 23/23 [00:19<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.66it/s]\n",
            "                   all         16         55      0.391      0.582      0.514      0.245\n",
            "     person-with-phone         16         18      0.428      0.833      0.764       0.44\n",
            "  person-without-phone         16         19      0.342      0.579      0.384      0.186\n",
            "         phone-posture         16         18      0.403      0.333      0.394       0.11\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/25      7.94G      2.003      2.169      1.756         56        640: 100% 23/23 [00:19<00:00,  1.20it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.68it/s]\n",
            "                   all         16         55      0.506      0.597      0.468      0.243\n",
            "     person-with-phone         16         18      0.419      0.778      0.687      0.389\n",
            "  person-without-phone         16         19      0.499      0.737       0.46      0.238\n",
            "         phone-posture         16         18      0.599      0.278      0.255      0.103\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/25      7.94G      2.007      2.159      1.747         62        640: 100% 23/23 [00:19<00:00,  1.17it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.69it/s]\n",
            "                   all         16         55      0.535      0.635      0.524      0.266\n",
            "     person-with-phone         16         18      0.536      0.778      0.686      0.429\n",
            "  person-without-phone         16         19      0.397      0.737      0.455      0.227\n",
            "         phone-posture         16         18      0.671      0.389       0.43      0.141\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/25      7.94G      1.966      2.038      1.728         36        640: 100% 23/23 [00:18<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.65it/s]\n",
            "                   all         16         55      0.531      0.654      0.554      0.262\n",
            "     person-with-phone         16         18      0.492      0.778      0.705      0.382\n",
            "  person-without-phone         16         19      0.467      0.684      0.474      0.236\n",
            "         phone-posture         16         18      0.633        0.5      0.481      0.169\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/25      7.94G      1.937      1.959      1.698         42        640: 100% 23/23 [00:19<00:00,  1.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.68it/s]\n",
            "                   all         16         55      0.488       0.58      0.517      0.253\n",
            "     person-with-phone         16         18      0.535      0.667      0.691      0.401\n",
            "  person-without-phone         16         19       0.41      0.684       0.43      0.234\n",
            "         phone-posture         16         18      0.518      0.389      0.428      0.125\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/25      7.94G      1.902      1.972      1.688         40        640: 100% 23/23 [00:18<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.73it/s]\n",
            "                   all         16         55      0.474      0.609      0.534      0.257\n",
            "     person-with-phone         16         18      0.497      0.778      0.646      0.358\n",
            "  person-without-phone         16         19      0.441      0.632      0.474      0.264\n",
            "         phone-posture         16         18      0.485      0.419      0.483      0.147\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/25      7.94G      1.922      1.942      1.676         56        640: 100% 23/23 [00:20<00:00,  1.10it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.69it/s]\n",
            "                   all         16         55      0.566      0.542      0.546      0.274\n",
            "     person-with-phone         16         18      0.499      0.722      0.701      0.367\n",
            "  person-without-phone         16         19      0.455      0.579      0.439      0.255\n",
            "         phone-posture         16         18      0.744      0.324      0.499      0.199\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/25      7.94G      1.883      1.891       1.82         19        640: 100% 23/23 [00:15<00:00,  1.53it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.65it/s]\n",
            "                   all         16         55      0.459      0.671      0.547      0.264\n",
            "     person-with-phone         16         18      0.425      0.833      0.703      0.358\n",
            "  person-without-phone         16         19      0.448      0.727      0.456       0.25\n",
            "         phone-posture         16         18      0.504      0.452      0.482      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/25      7.94G      1.846      1.821      1.799         24        640: 100% 23/23 [00:13<00:00,  1.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.70it/s]\n",
            "                   all         16         55      0.523      0.668      0.582      0.274\n",
            "     person-with-phone         16         18      0.666      0.774      0.678      0.299\n",
            "  person-without-phone         16         19      0.418      0.842      0.594      0.315\n",
            "         phone-posture         16         18      0.485      0.389      0.474      0.208\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/25      7.94G      1.811      1.746      1.744         24        640: 100% 23/23 [00:13<00:00,  1.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.73it/s]\n",
            "                   all         16         55      0.489      0.637      0.595      0.308\n",
            "     person-with-phone         16         18      0.482      0.833      0.733       0.37\n",
            "  person-without-phone         16         19      0.466       0.69      0.529      0.317\n",
            "         phone-posture         16         18      0.519      0.389      0.522      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/25      7.94G      1.762      1.721      1.735         24        640: 100% 23/23 [00:13<00:00,  1.75it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.28it/s]\n",
            "                   all         16         55      0.545      0.636      0.575      0.303\n",
            "     person-with-phone         16         18       0.57      0.778      0.623      0.347\n",
            "  person-without-phone         16         19      0.515      0.632       0.52       0.31\n",
            "         phone-posture         16         18       0.55        0.5       0.58      0.251\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/25      7.94G      1.787      1.641      1.763         27        640: 100% 23/23 [00:12<00:00,  1.82it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.27it/s]\n",
            "                   all         16         55      0.601       0.69      0.628      0.331\n",
            "     person-with-phone         16         18      0.629      0.833      0.736      0.409\n",
            "  person-without-phone         16         19      0.462      0.737      0.568      0.343\n",
            "         phone-posture         16         18      0.713        0.5       0.58       0.24\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      21/25      7.94G      1.784      1.593      1.747         26        640: 100% 23/23 [00:12<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.72it/s]\n",
            "                   all         16         55      0.531       0.67      0.628      0.303\n",
            "     person-with-phone         16         18      0.536      0.833      0.734      0.351\n",
            "  person-without-phone         16         19      0.541      0.622      0.557      0.321\n",
            "         phone-posture         16         18      0.514      0.556      0.594      0.236\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      22/25      7.94G      1.715       1.56      1.709         23        640: 100% 23/23 [00:13<00:00,  1.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.72it/s]\n",
            "                   all         16         55      0.619      0.672      0.637      0.313\n",
            "     person-with-phone         16         18      0.686      0.728      0.778      0.375\n",
            "  person-without-phone         16         19      0.504      0.789      0.505      0.321\n",
            "         phone-posture         16         18      0.669        0.5      0.627      0.242\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      23/25      7.94G      1.643      1.498      1.635         17        640: 100% 23/23 [00:12<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.70it/s]\n",
            "                   all         16         55      0.665      0.672       0.69      0.333\n",
            "     person-with-phone         16         18      0.677      0.667      0.756      0.366\n",
            "  person-without-phone         16         19      0.571      0.737      0.591      0.368\n",
            "         phone-posture         16         18      0.747      0.611      0.724      0.264\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      24/25      7.94G      1.638      1.448      1.654         19        640: 100% 23/23 [00:12<00:00,  1.80it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.67it/s]\n",
            "                   all         16         55      0.735       0.63      0.687      0.343\n",
            "     person-with-phone         16         18      0.754      0.683      0.777      0.396\n",
            "  person-without-phone         16         19      0.557      0.737        0.6      0.377\n",
            "         phone-posture         16         18      0.894      0.471      0.684      0.255\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      25/25      7.94G      1.627      1.435      1.618         27        640: 100% 23/23 [00:12<00:00,  1.81it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:00<00:00,  1.45it/s]\n",
            "                   all         16         55      0.608      0.636      0.677      0.335\n",
            "     person-with-phone         16         18      0.521      0.725      0.765      0.397\n",
            "  person-without-phone         16         19      0.517      0.684      0.597      0.348\n",
            "         phone-posture         16         18      0.787        0.5      0.669      0.261\n",
            "\n",
            "25 epochs completed in 0.136 hours.\n",
            "Optimizer stripped from runs/detect/train/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from runs/detect/train/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating runs/detect/train/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:01<00:00,  1.02s/it]\n",
            "                   all         16         55      0.735       0.63      0.687      0.343\n",
            "     person-with-phone         16         18      0.754      0.683      0.777      0.396\n",
            "  person-without-phone         16         19      0.558      0.737        0.6      0.377\n",
            "         phone-posture         16         18      0.894      0.471      0.683      0.255\n",
            "Speed: 0.2ms pre-process, 13.4ms inference, 0.0ms loss, 1.6ms post-process per image\n",
            "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls runs/detect/train/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB-ZXdlGyVXd",
        "outputId": "de2b174c-5a9a-4ac4-9108-78d20dd7b90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.yaml\t\t\t\t\t    R_curve.png       train_batch345.jpg\n",
            "confusion_matrix.png\t\t\t\t    results.csv       train_batch346.jpg\n",
            "events.out.tfevents.1712245542.f663d59dd295.4996.0  results.png       train_batch347.jpg\n",
            "F1_curve.png\t\t\t\t\t    train_batch0.jpg  val_batch0_labels.jpg\n",
            "P_curve.png\t\t\t\t\t    train_batch1.jpg  val_batch0_pred.jpg\n",
            "PR_curve.png\t\t\t\t\t    train_batch2.jpg  weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=val model=runs/detect/train/weights/best.pt data=/content/Actual-project-1/data.yaml\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro5tKek20zQH",
        "outputId": "44057789-6f29-4e2b-c125-b49f5943b7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 15:57:44.677701: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 15:57:44.677753: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 15:57:44.679128: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 15:57:45.866874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Actual-project-1/valid/labels.cache... 16 images, 0 backgrounds, 0 corrupt: 100% 16/16 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 1/1 [00:01<00:00,  1.47s/it]\n",
            "                   all         16         55      0.735       0.63      0.687      0.341\n",
            "     person-with-phone         16         18      0.754      0.683      0.777      0.396\n",
            "  person-without-phone         16         19      0.558      0.737        0.6      0.377\n",
            "         phone-posture         16         18      0.894       0.47      0.683      0.251\n",
            "Speed: 0.3ms pre-process, 22.8ms inference, 0.0ms loss, 33.4ms post-process per image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo task=detect mode=predict model=runs/detect/train/weights/best.pt conf=0.25 source=/content/Actual-project-1/test/images save=True\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb9-P47I1HNm",
        "outputId": "8871254f-f419-4b90-aba8-4adb6e5f8266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-04 16:26:24.330147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-04 16:26:24.330204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-04 16:26:24.331800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-04 16:26:25.828457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n",
            "image 1/35 /content/Actual-project-1/test/images/20240313_122136_mp4-0008_jpg.rf.611019c9a85ab29f71f8c5fde8024048.jpg: 640x640 2 person-with-phones, 1 phone-posture, 37.1ms\n",
            "image 2/35 /content/Actual-project-1/test/images/20240313_122248_mp4-0000_jpg.rf.c12f6d6382a3f32a68771b34d2052884.jpg: 640x640 1 person-without-phone, 37.2ms\n",
            "image 3/35 /content/Actual-project-1/test/images/20240313_122825_mp4-0001_jpg.rf.10a83a62943f340654785df4f43a1320.jpg: 640x640 1 person-without-phone, 37.0ms\n",
            "image 4/35 /content/Actual-project-1/test/images/20240313_122825_mp4-0002_jpg.rf.fceb8ca1213e2bbc28b35e6b7f63edf0.jpg: 640x640 37.0ms\n",
            "image 5/35 /content/Actual-project-1/test/images/20240313_124243_mp4-0008_jpg.rf.f7a0fff5634ad0b6e06dc17a3d9221b6.jpg: 640x640 1 person-with-phone, 1 phone-posture, 37.0ms\n",
            "image 6/35 /content/Actual-project-1/test/images/20240316_171432_jpg.rf.a72f49b7a6bb67407986aa5284a0a69c.jpg: 640x640 1 person-with-phone, 2 person-without-phones, 26.1ms\n",
            "image 7/35 /content/Actual-project-1/test/images/20240316_171655_jpg.rf.869e75784b278704f53792ea897fc8a2.jpg: 640x640 2 person-with-phones, 1 person-without-phone, 24.4ms\n",
            "image 8/35 /content/Actual-project-1/test/images/20240316_171702_jpg.rf.96a921d1fe63e1c5d3b0a7711ba2be75.jpg: 640x640 5 person-without-phones, 24.4ms\n",
            "image 9/35 /content/Actual-project-1/test/images/20240316_171713_jpg.rf.dc0ca35cec4604563d7b37d8fe0c5020.jpg: 640x640 1 person-with-phone, 5 person-without-phones, 24.0ms\n",
            "image 10/35 /content/Actual-project-1/test/images/20240316_171740_jpg.rf.f9cfe6986ddbae387c79588d12bfc40d.jpg: 640x640 4 person-with-phones, 5 person-without-phones, 1 phone-posture, 24.0ms\n",
            "image 11/35 /content/Actual-project-1/test/images/20240316_171805_jpg.rf.5e881e78c2007994ca6dd398c0b5c4e0.jpg: 640x640 1 person-without-phone, 18.4ms\n",
            "image 12/35 /content/Actual-project-1/test/images/20240316_171948_jpg.rf.0b29875573545f366dfaa21459d9c4b2.jpg: 640x640 1 person-with-phone, 1 person-without-phone, 18.4ms\n",
            "image 13/35 /content/Actual-project-1/test/images/20240316_172159_jpg.rf.f3424a91ab08fb47e14f974c3012e9c5.jpg: 640x640 1 person-with-phone, 7 person-without-phones, 18.4ms\n",
            "image 14/35 /content/Actual-project-1/test/images/20240316_172426_jpg.rf.bf6f48e4e00df0bd82284881251b3b74.jpg: 640x640 2 person-with-phones, 1 phone-posture, 18.4ms\n",
            "image 15/35 /content/Actual-project-1/test/images/20240316_172524_jpg.rf.d6162baaaa6f18080216fc43b0928329.jpg: 640x640 2 person-with-phones, 3 person-without-phones, 1 phone-posture, 18.5ms\n",
            "image 16/35 /content/Actual-project-1/test/images/20240316_172729_jpg.rf.a4510e14c8399e182a9ba1082421077d.jpg: 640x640 1 person-with-phone, 2 person-without-phones, 1 phone-posture, 18.4ms\n",
            "image 17/35 /content/Actual-project-1/test/images/20240316_172737_jpg.rf.282835d87bff9e844768afd0fbff5da3.jpg: 640x640 3 person-with-phones, 2 person-without-phones, 1 phone-posture, 18.5ms\n",
            "image 18/35 /content/Actual-project-1/test/images/20240316_172740_jpg.rf.13a4edc39c04c1431106b506334374f8.jpg: 640x640 2 person-with-phones, 2 person-without-phones, 18.0ms\n",
            "image 19/35 /content/Actual-project-1/test/images/20240316_173707_jpg.rf.4ec69a75eeb4c168fb746dae2e08cf40.jpg: 640x640 1 person-with-phone, 2 person-without-phones, 17.8ms\n",
            "image 20/35 /content/Actual-project-1/test/images/IMG-20240314-WA0002_jpg.rf.53ce41e5f5c9f3baa0db9a332b4b4dda.jpg: 640x640 2 person-with-phones, 2 person-without-phones, 17.8ms\n",
            "image 21/35 /content/Actual-project-1/test/images/IMG-20240314-WA0006_jpg.rf.8f5bd719b1f69e18219ada23a46b9d83.jpg: 640x640 3 person-without-phones, 17.2ms\n",
            "image 22/35 /content/Actual-project-1/test/images/IMG-20240314-WA0007_jpg.rf.8e78478d7d5227ea37e38990f2b9115b.jpg: 640x640 3 person-without-phones, 17.4ms\n",
            "image 23/35 /content/Actual-project-1/test/images/IMG-20240314-WA0015_jpg.rf.0f50d9041397c47a8c7ce2be8a51ec08.jpg: 640x640 4 person-without-phones, 17.4ms\n",
            "image 24/35 /content/Actual-project-1/test/images/IMG20240316171134_jpg.rf.7b10aebc23ac9fd66d04d4f80e07912b.jpg: 640x640 1 person-with-phone, 17.7ms\n",
            "image 25/35 /content/Actual-project-1/test/images/IMG20240316171202_jpg.rf.e19936375db2cdc699d493cbf2cadd36.jpg: 640x640 1 person-with-phone, 18.0ms\n",
            "image 26/35 /content/Actual-project-1/test/images/IMG20240316171432_jpg.rf.5bca197ed533199767da4d299f83d3b5.jpg: 640x640 1 person-with-phone, 2 person-without-phones, 18.8ms\n",
            "image 27/35 /content/Actual-project-1/test/images/IMG20240316171533_jpg.rf.a36dae590690bda8e2a23016178eeee2.jpg: 640x640 1 person-with-phone, 1 phone-posture, 18.1ms\n",
            "image 28/35 /content/Actual-project-1/test/images/IMG20240316172053_jpg.rf.6a818bacd64e3e52072091c0455e502d.jpg: 640x640 1 person-with-phone, 4 person-without-phones, 1 phone-posture, 17.5ms\n",
            "image 29/35 /content/Actual-project-1/test/images/IMG20240316172815_jpg.rf.65b703bccef02f362070758bd1b18015.jpg: 640x640 1 person-with-phone, 1 phone-posture, 17.7ms\n",
            "image 30/35 /content/Actual-project-1/test/images/IMG20240316180004_jpg.rf.b40da5444419054dd01d9ab515bea065.jpg: 640x640 1 person-with-phone, 1 phone-posture, 18.2ms\n",
            "image 31/35 /content/Actual-project-1/test/images/IMG_20240314_113800_jpg.rf.334785ddd46881e80aff1bba7d0c2ce8.jpg: 640x640 1 person-without-phone, 18.8ms\n",
            "image 32/35 /content/Actual-project-1/test/images/IMG_20240314_113926_jpg.rf.03c8756f660d7992b7573a4297a77e03.jpg: 640x640 1 person-with-phone, 1 phone-posture, 18.1ms\n",
            "image 33/35 /content/Actual-project-1/test/images/IMG_20240314_175228_jpg.rf.6c2c68d376d6b3f6876bc10e9831f739.jpg: 640x640 17.6ms\n",
            "image 34/35 /content/Actual-project-1/test/images/IMG_20240314_175341_jpg.rf.b99f23f9e30a0128782cb63b2d5e7f4b.jpg: 640x640 1 person-with-phone, 1 person-without-phone, 1 phone-posture, 17.4ms\n",
            "image 35/35 /content/Actual-project-1/test/images/IMG_20240314_175403_jpg.rf.cdd9aaa0dda12b31a178adb177deefb9.jpg: 640x640 18.3ms\n",
            "Speed: 0.5ms pre-process, 21.7ms inference, 17.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for image_path in glob.glob('runs/detect/predict/*.jpg')[:3]:\n",
        "      display(Image(filename=image_path, width=600))\n",
        "      print(\"\\n\")"
      ],
      "metadata": {
        "id": "Xb3i8BWZ1Su_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')  # load a pretrained YOLOv8n detection model\n",
        "model.train(data='/content/Actual-project-1/data.yaml', epochs=3)  # train the model\n",
        "model('/content/Actual-project-1/test/images/20240313_122136_mp4-0008_jpg.rf.611019c9a85ab29f71f8c5fde8024048.jpg')  # predict on an image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9IAGUIU112w",
        "outputId": "c5286f37-8f4e-45c0-9fbb-3186d7eb0941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.yaml, data=/content/Actual-project-1/data.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.1, copy_paste=0.1, cfg=None, v5loader=False, save_dir=runs/detect/train4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1392  ultralytics.nn.modules.Conv                  [3, 48, 3, 2]                 \n",
            "  1                  -1  1     41664  ultralytics.nn.modules.Conv                  [48, 96, 3, 2]                \n",
            "  2                  -1  2    111360  ultralytics.nn.modules.C2f                   [96, 96, 2, True]             \n",
            "  3                  -1  1    166272  ultralytics.nn.modules.Conv                  [96, 192, 3, 2]               \n",
            "  4                  -1  4    813312  ultralytics.nn.modules.C2f                   [192, 192, 4, True]           \n",
            "  5                  -1  1    664320  ultralytics.nn.modules.Conv                  [192, 384, 3, 2]              \n",
            "  6                  -1  4   3248640  ultralytics.nn.modules.C2f                   [384, 384, 4, True]           \n",
            "  7                  -1  1   1991808  ultralytics.nn.modules.Conv                  [384, 576, 3, 2]              \n",
            "  8                  -1  2   3985920  ultralytics.nn.modules.C2f                   [576, 576, 2, True]           \n",
            "  9                  -1  1    831168  ultralytics.nn.modules.SPPF                  [576, 576, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  2   1993728  ultralytics.nn.modules.C2f                   [960, 384, 2]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  2    517632  ultralytics.nn.modules.C2f                   [576, 192, 2]                 \n",
            " 16                  -1  1    332160  ultralytics.nn.modules.Conv                  [192, 192, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  2   1846272  ultralytics.nn.modules.C2f                   [576, 384, 2]                 \n",
            " 19                  -1  1   1327872  ultralytics.nn.modules.Conv                  [384, 384, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  2   4207104  ultralytics.nn.modules.C2f                   [960, 576, 2]                 \n",
            " 22        [15, 18, 21]  1   3777433  ultralytics.nn.modules.Detect                [3, [192, 384, 576]]          \n",
            "Model summary: 295 layers, 25858057 parameters, 25858041 gradients, 79.1 GFLOPs\n",
            "\n",
            "Transferred 475/475 items from pretrained weights\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.001), 83 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/Actual-project-1/train/labels.cache... 360 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 360/360 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/Actual-project-1/valid/labels.cache... 16 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<?, ?it/s]\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3      7.53G      1.778      1.699      1.611         49        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:30<00:00,  1.30s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.39it/s]\n",
            "                   all         16         55      0.774      0.687       0.73       0.38\n",
            "     person-with-phone         16         18      0.801      0.778       0.81       0.43\n",
            "  person-without-phone         16         19      0.557      0.728      0.629      0.385\n",
            "         phone-posture         16         18      0.964      0.556       0.75      0.324\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3      7.53G      1.724      1.609      1.561         56        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:20<00:00,  1.12it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.61it/s]\n",
            "                   all         16         55      0.607      0.725      0.665      0.315\n",
            "     person-with-phone         16         18      0.666      0.774      0.749      0.384\n",
            "  person-without-phone         16         19      0.452      0.789       0.54      0.336\n",
            "         phone-posture         16         18      0.702      0.611      0.705      0.225\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3      7.54G      1.657      1.529      1.515         63        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23/23 [00:19<00:00,  1.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.30it/s]\n",
            "                   all         16         55      0.699      0.575      0.691      0.352\n",
            "     person-with-phone         16         18      0.729       0.75      0.801      0.439\n",
            "  person-without-phone         16         19       0.47      0.421      0.534      0.336\n",
            "         phone-posture         16         18      0.897      0.556      0.739      0.282\n",
            "\n",
            "3 epochs completed in 0.022 hours.\n",
            "Optimizer stripped from runs/detect/train4/weights/last.pt, 52.0MB\n",
            "Optimizer stripped from runs/detect/train4/weights/best.pt, 52.0MB\n",
            "\n",
            "Validating runs/detect/train4/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.28it/s]\n",
            "                   all         16         55      0.774      0.687       0.73       0.38\n",
            "     person-with-phone         16         18      0.801      0.778       0.81       0.43\n",
            "  person-without-phone         16         19      0.557      0.728      0.629      0.385\n",
            "         phone-posture         16         18      0.964      0.556       0.75      0.325\n",
            "Speed: 0.2ms pre-process, 12.1ms inference, 0.0ms loss, 1.3ms post-process per image\n",
            "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.10.12 torch-2.2.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 218 layers, 25841497 parameters, 0 gradients, 78.7 GFLOPs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Ultralytics YOLO <class 'ultralytics.yolo.engine.results.Boxes'> masks\n",
              " type: <class 'torch.Tensor'>\n",
              " shape: torch.Size([4, 6])\n",
              " dtype: torch.float32\n",
              "  + tensor([[1.69000e+02, 1.81000e+02, 3.41000e+02, 2.92000e+02, 6.22457e-01, 0.00000e+00],\n",
              "         [1.98000e+02, 1.90000e+02, 2.72000e+02, 2.54000e+02, 4.84260e-01, 2.00000e+00],\n",
              "         [1.89000e+02, 1.79000e+02, 3.11000e+02, 2.86000e+02, 4.68847e-01, 0.00000e+00],\n",
              "         [1.98000e+02, 1.87000e+02, 2.56000e+02, 2.47000e+02, 3.07587e-01, 2.00000e+00]], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZueksmFE3Dls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}